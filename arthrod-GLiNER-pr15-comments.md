# PR Comments Export

> Exported from [https://github.com/arthrod/GLiNER/pull/15](https://github.com/arthrod/GLiNER/pull/15)  
> 7 of 8 comments selected  
> Generated by [Cicero](https://cicero.im) on 2/17/2026, 8:22:21 PM

---

### gemini-code-assist[bot] &mdash; 2/17/2026, 7:11:57 PM

> File: `tests/test_training_validation.py`

<details><summary>Code context</summary>

```diff
@@ -0,0 +1,684 @@
+"""Tests for training parameter validation and configuration wiring.
+
+These tests verify that training parameters are correctly defined, forwarded,
+and defaulted throughout the GLiNER training pipeline. They catch issues
+identified in the training parameters and loss configuration report.
+
+Tests marked with ``@pytest.mark.xfail(strict=True)`` document known gaps.
+When a gap is fixed the test will unexpectedly pass (XPASS) and the marker
+should be removed.
+
+Test categories:
+    - TrainingArguments field definitions and defaults
+    - create_training_args explicit parameter coverage
+    - train_model parameter forwarding
+    - Config field consumption (dead fields)
+    - Label smoothing name collision risk
+"""
+
+import inspect
+from dataclasses import fields as dataclass_fields
+from pathlib import Path
+
+import pytest
+import transformers
+
+from gliner.training.trainer import TrainingArguments, Trainer
+
+
+# ---------------------------------------------------------------------------
+# Helpers
+# ---------------------------------------------------------------------------
+
+
+def _get_create_training_args_explicit_params():
+    """Return the set of explicit parameter names in create_training_args.
+
+    Imports BaseGLiNER and inspects the classmethod signature, excluding
+    'cls', 'self', and '**kwargs'.
+    """
+    from gliner.model import BaseGLiNER
+
+    sig = inspect.signature(BaseGLiNER.create_training_args)
+    return {
+        name
+        for name, param in sig.parameters.items()
+        if name not in ("cls", "self")
+        and param.kind
+        not in (
+            inspect.Parameter.VAR_KEYWORD,
+            inspect.Parameter.VAR_POSITIONAL,
+        )
+    }
+
+
+def _get_training_args_field_names():
+    """Return all field names defined on the custom TrainingArguments dataclass."""
+    return {f.name for f in dataclass_fields(TrainingArguments)}
+
+
+def _make_training_args(**overrides):
+    """Create a TrainingArguments with a temporary output_dir and no reporting."""
+    defaults = {
+        "output_dir": "/tmp/gliner_test_output",
```
</details>

![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

Using a hardcoded temporary directory path like `/tmp/gliner_test_output` is not ideal for tests as it can lead to conflicts if tests are run in parallel and does not guarantee cleanup. It's better to use pytest's `tmp_path` fixture to create a unique, temporary directory for each test run.

I recommend refactoring this helper function (and `_create_training_args_via_classmethod`) to require an `output_dir` argument, which can then be supplied by tests using `tmp_path`. This makes dependencies clear and tests more robust.

For example:

```python
def _make_training_args(output_dir: str, **overrides):
    # ...

# And in your tests:
def test_training_args_instantiation(self, tmp_path):
    args = _make_training_args(str(tmp_path))
    # ...
```
This change should also be applied to other places with hardcoded paths, like in `_create_training_args_via_classmethod` and `test_hf_remove_unused_columns_defaults_to_true`.

---

### gemini-code-assist[bot] &mdash; 2/17/2026, 7:11:57 PM

> File: `tests/test_training_validation.py`

<details><summary>Code context</summary>

```diff
@@ -0,0 +1,684 @@
+"""Tests for training parameter validation and configuration wiring.
+
+These tests verify that training parameters are correctly defined, forwarded,
+and defaulted throughout the GLiNER training pipeline. They catch issues
+identified in the training parameters and loss configuration report.
+
+Tests marked with ``@pytest.mark.xfail(strict=True)`` document known gaps.
+When a gap is fixed the test will unexpectedly pass (XPASS) and the marker
+should be removed.
+
+Test categories:
+    - TrainingArguments field definitions and defaults
+    - create_training_args explicit parameter coverage
+    - train_model parameter forwarding
+    - Config field consumption (dead fields)
+    - Label smoothing name collision risk
+"""
+
+import inspect
+from dataclasses import fields as dataclass_fields
+from pathlib import Path
+
+import pytest
+import transformers
+
+from gliner.training.trainer import TrainingArguments, Trainer
+
+
+# ---------------------------------------------------------------------------
+# Helpers
+# ---------------------------------------------------------------------------
+
+
+def _get_create_training_args_explicit_params():
+    """Return the set of explicit parameter names in create_training_args.
+
+    Imports BaseGLiNER and inspects the classmethod signature, excluding
+    'cls', 'self', and '**kwargs'.
+    """
+    from gliner.model import BaseGLiNER
+
+    sig = inspect.signature(BaseGLiNER.create_training_args)
+    return {
+        name
+        for name, param in sig.parameters.items()
+        if name not in ("cls", "self")
+        and param.kind
+        not in (
+            inspect.Parameter.VAR_KEYWORD,
+            inspect.Parameter.VAR_POSITIONAL,
+        )
+    }
+
+
+def _get_training_args_field_names():
+    """Return all field names defined on the custom TrainingArguments dataclass."""
+    return {f.name for f in dataclass_fields(TrainingArguments)}
+
+
+def _make_training_args(**overrides):
+    """Create a TrainingArguments with a temporary output_dir and no reporting."""
+    defaults = {
+        "output_dir": "/tmp/gliner_test_output",
+        "report_to": "none",
+        "use_cpu": True,
+    }
+    defaults.update(overrides)
+    return TrainingArguments(**defaults)
+
+
+def _create_training_args_via_classmethod(**overrides):
+    """Create TrainingArguments through BaseGLiNER.create_training_args."""
+    from gliner.model import BaseGLiNER
+
+    defaults = {
+        "output_dir": "/tmp/gliner_test_output",
+        "report_to": "none",
+    }
+    defaults.update(overrides)
+    return BaseGLiNER.create_training_args(**defaults)
+
+
+def _get_train_model_signature():
+    """Return the signature of BaseGLiNER.train_model."""
+    from gliner.model import BaseGLiNER
+
+    return inspect.signature(BaseGLiNER.train_model)
+
+
+def _get_train_model_source():
+    """Return the source code of BaseGLiNER.train_model."""
+    from gliner.model import BaseGLiNER
+
+    return inspect.getsource(BaseGLiNER.train_model)
+
+
+# ===========================================================================
+# 1. Smoke tests  --  these MUST pass
+# ===========================================================================
+
+
+class TestSmoke:
+    """Basic smoke tests for training module imports and instantiation."""
+
+    def test_training_args_import(self):
+        from gliner.training import TrainingArguments as TA
+
+        assert TA is not None
+
+    def test_trainer_import(self):
+        from gliner.training import Trainer as T
+
+        assert T is not None
+
+    def test_training_args_instantiation(self):
+        args = _make_training_args()
+        assert args.output_dir == "/tmp/gliner_test_output"
+
+    def test_create_training_args_callable(self):
+        from gliner.model import BaseGLiNER
+
+        assert callable(BaseGLiNER.create_training_args)
+
+    def test_create_training_args_returns_training_args(self):
+        args = _create_training_args_via_classmethod()
+        assert isinstance(args, TrainingArguments)
+
+
+# ===========================================================================
+# 2. TrainingArguments defaults  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainingArgumentsDefaults:
+    """Verify TrainingArguments field defaults match expected values."""
+
+    def test_masking_default_is_global(self):
+        """TrainingArguments.masking should default to 'global'."""
+        args = _make_training_args()
+        assert args.masking == "global"
+
+    def test_label_smoothing_field_exists(self):
+        field_names = _get_training_args_field_names()
+        assert "label_smoothing" in field_names
+
+    def test_loss_reduction_default_is_sum(self):
+        args = _make_training_args()
+        assert args.loss_reduction == "sum"
+
+    def test_focal_loss_defaults(self):
+        args = _make_training_args()
+        assert args.focal_loss_alpha == -1
+        assert args.focal_loss_gamma == 0
+        assert args.focal_loss_prob_margin == 0
+
+
+# ===========================================================================
+# 3. TrainingArguments custom fields completeness  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainingArgumentsFieldCompleteness:
+    """Verify all GLiNER-specific fields are properly declared."""
+
+    REQUIRED_GLINER_FIELDS = {
+        "focal_loss_alpha",
+        "focal_loss_gamma",
+        "focal_loss_prob_margin",
+        "label_smoothing",
+        "loss_reduction",
+        "negatives",
+        "masking",
+        "others_lr",
+        "others_weight_decay",
+    }
+
+    def test_all_gliner_fields_declared(self):
+        field_names = _get_training_args_field_names()
+        missing = self.REQUIRED_GLINER_FIELDS - field_names
+        assert not missing, f"Missing GLiNER-specific fields: {missing}"
+
+    def test_gliner_fields_have_defaults(self):
+        """All GLiNER-specific fields should have defaults (instantiation with only output_dir)."""
+        try:
+            _make_training_args()
+        except TypeError as e:
+            pytest.fail(f"TrainingArguments cannot be instantiated with defaults only: {e}")
+
+
+# ===========================================================================
+# 4. Compute loss wiring  --  this MUST pass
+# ===========================================================================
+
+
+class TestComputeLossWiring:
+    """Verify Trainer.compute_loss correctly forwards all loss parameters."""
+
+    def test_compute_loss_passes_all_loss_params(self):
+        source = inspect.getsource(Trainer.compute_loss)
+        required_forwards = [
+            ("alpha", "self.args.focal_loss_alpha"),
+            ("gamma", "self.args.focal_loss_gamma"),
+            ("prob_margin", "self.args.focal_loss_prob_margin"),
+            ("label_smoothing", "self.args.label_smoothing"),
+            ("reduction", "self.args.loss_reduction"),
+            ("negatives", "self.args.negatives"),
+            ("masking", "self.args.masking"),
+        ]
+        missing = []
+        for param_name, attr_path in required_forwards:
+            if attr_path not in source:
+                missing.append(f"{param_name} (via {attr_path})")
+        assert not missing, f"compute_loss does not forward: {', '.join(missing)}"
+
+
+# ===========================================================================
+# 5. Trainer dataloader wiring  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainerDataloaderWiring:
+    """Verify the Trainer correctly uses dataloader configuration from args."""
+
+    def test_get_train_dataloader_uses_pin_memory(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_pin_memory" in source
+
+    def test_get_train_dataloader_uses_persistent_workers(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_persistent_workers" in source
+
+    def test_get_train_dataloader_uses_prefetch_factor(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_prefetch_factor" in source
+
+
+# ===========================================================================
+# 6. kwargs pass-through integrity  --  these MUST pass
+# ===========================================================================
+
+
+class TestKwargsPassThrough:
+    """Verify kwargs passed to create_training_args reach TrainingArguments."""
+
+    def test_label_smoothing_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(label_smoothing=0.1)
+        assert args.label_smoothing == 0.1
+
+    def test_fp16_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(fp16=True)
+        assert args.fp16 is True
+
+    def test_seed_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(seed=42)
+        assert args.seed == 42
+
+
+# ===========================================================================
+# 7. HF default for remove_unused_columns  --  documents the risk
+# ===========================================================================
+
+
+class TestHFDefaults:
+    """Document HF default values that create risk for GLiNER."""
+
+    def test_hf_remove_unused_columns_defaults_to_true(self):
+        """HF Trainer defaults remove_unused_columns to True.
+
+        This is dangerous for GLiNER which uses custom batch dictionaries.
+        """
+        hf_args = transformers.TrainingArguments(
+            output_dir="/tmp/test", report_to="none"
+        )
+        assert hf_args.remove_unused_columns is True
+
+    def test_hf_label_smoothing_factor_defaults_to_zero(self):
+        """HF label_smoothing_factor defaults to 0 (no double smoothing by default)."""
+        args = _make_training_args(label_smoothing=0.1)
+        hf_ls = getattr(args, "label_smoothing_factor", 0)
+        assert hf_ls == 0
+
+
+# ===========================================================================
+#
+#   BUG-CATCHING TESTS  --  xfail(strict=True)
+#
+#   These tests assert the CORRECT behavior. They currently fail because the
+#   code has known issues. When an issue is fixed, the test will XPASS and
+#   the marker should be removed.
+#
+# ===========================================================================
+
+
+# ---------------------------------------------------------------------------
+# 8. create_training_args explicit parameter coverage gaps
+# ---------------------------------------------------------------------------
+
+
+class TestCreateTrainingArgsSignature:
+    """Verify create_training_args has explicit parameters for critical fields.
+
+    Fields relying on **kwargs are fragile: not discoverable, not documented
+    in the signature, and can silently break if TrainingArguments changes.
+    """
+
+    @pytest.fixture(scope="class")
+    def explicit_params(self):
+        return _get_create_training_args_explicit_params()
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="label_smoothing relies on **kwargs; not explicit in create_training_args",
+    )
+    def test_label_smoothing_is_explicit(self, explicit_params):
+        assert "label_smoothing" in explicit_params, (
+            "label_smoothing is not an explicit parameter in create_training_args; "
+            "it relies on **kwargs pass-through which is fragile"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="fp16 is not explicit; only bf16 is, creating an asymmetry",
+    )
+    def test_fp16_is_explicit(self, explicit_params):
+        assert "fp16" in explicit_params, (
+            "fp16 is not explicit in create_training_args; only bf16 is"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="seed not explicit; Trainer's internal seed may differ from user's",
+    )
+    def test_seed_is_explicit(self, explicit_params):
+        assert "seed" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="gradient_checkpointing not explicit; critical for large models",
+    )
+    def test_gradient_checkpointing_is_explicit(self, explicit_params):
+        assert "gradient_checkpointing" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="run_name not explicit; experiment trackers get auto-generated names",
+    )
+    def test_run_name_is_explicit(self, explicit_params):
+        assert "run_name" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="push_to_hub not explicit in create_training_args",
+    )
+    def test_push_to_hub_is_explicit(self, explicit_params):
+        assert "push_to_hub" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="hub_model_id not explicit in create_training_args",
+    )
+    def test_hub_model_id_is_explicit(self, explicit_params):
+        assert "hub_model_id" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="evaluation_strategy/eval_strategy not explicit; evaluation never runs",
+    )
+    def test_evaluation_strategy_is_explicit(self, explicit_params):
+        has_eval = (
+            "evaluation_strategy" in explicit_params
+            or "eval_strategy" in explicit_params
+        )
+        assert has_eval, (
+            "Neither evaluation_strategy nor eval_strategy is explicit; "
+            "evaluation never runs during training"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="eval_steps not explicit; evaluation frequency cannot be controlled",
+    )
+    def test_eval_steps_is_explicit(self, explicit_params):
+        assert "eval_steps" in explicit_params
+
+
+# ---------------------------------------------------------------------------
+# 9. Masking default mismatch
+# ---------------------------------------------------------------------------
+
+
+class TestMaskingDefaultMismatch:
+    """Detect default mismatch between create_training_args and TrainingArguments."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "create_training_args defaults masking to 'none' but "
+            "TrainingArguments defaults to 'global'"
+        ),
+    )
+    def test_create_training_args_masking_matches_training_args_default(self):
+        """The two defaults should agree so create_training_args doesn't
+        silently override the GLiNER default.
+        """
+        sig = inspect.signature(_create_training_args_via_classmethod)
```
</details>

![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The variable `sig` is assigned but never used. The subsequent code correctly uses `real_sig` to inspect the actual method. This unused variable is dead code and can be removed to improve clarity.

---

### gemini-code-assist[bot] &mdash; 2/17/2026, 7:11:57 PM

> File: `tests/test_training_validation.py`

<details><summary>Code context</summary>

```diff
@@ -0,0 +1,684 @@
+"""Tests for training parameter validation and configuration wiring.
+
+These tests verify that training parameters are correctly defined, forwarded,
+and defaulted throughout the GLiNER training pipeline. They catch issues
+identified in the training parameters and loss configuration report.
+
+Tests marked with ``@pytest.mark.xfail(strict=True)`` document known gaps.
+When a gap is fixed the test will unexpectedly pass (XPASS) and the marker
+should be removed.
+
+Test categories:
+    - TrainingArguments field definitions and defaults
+    - create_training_args explicit parameter coverage
+    - train_model parameter forwarding
+    - Config field consumption (dead fields)
+    - Label smoothing name collision risk
+"""
+
+import inspect
+from dataclasses import fields as dataclass_fields
+from pathlib import Path
+
+import pytest
+import transformers
+
+from gliner.training.trainer import TrainingArguments, Trainer
+
+
+# ---------------------------------------------------------------------------
+# Helpers
+# ---------------------------------------------------------------------------
+
+
+def _get_create_training_args_explicit_params():
+    """Return the set of explicit parameter names in create_training_args.
+
+    Imports BaseGLiNER and inspects the classmethod signature, excluding
+    'cls', 'self', and '**kwargs'.
+    """
+    from gliner.model import BaseGLiNER
+
+    sig = inspect.signature(BaseGLiNER.create_training_args)
+    return {
+        name
+        for name, param in sig.parameters.items()
+        if name not in ("cls", "self")
+        and param.kind
+        not in (
+            inspect.Parameter.VAR_KEYWORD,
+            inspect.Parameter.VAR_POSITIONAL,
+        )
+    }
+
+
+def _get_training_args_field_names():
+    """Return all field names defined on the custom TrainingArguments dataclass."""
+    return {f.name for f in dataclass_fields(TrainingArguments)}
+
+
+def _make_training_args(**overrides):
+    """Create a TrainingArguments with a temporary output_dir and no reporting."""
+    defaults = {
+        "output_dir": "/tmp/gliner_test_output",
+        "report_to": "none",
+        "use_cpu": True,
+    }
+    defaults.update(overrides)
+    return TrainingArguments(**defaults)
+
+
+def _create_training_args_via_classmethod(**overrides):
+    """Create TrainingArguments through BaseGLiNER.create_training_args."""
+    from gliner.model import BaseGLiNER
+
+    defaults = {
+        "output_dir": "/tmp/gliner_test_output",
+        "report_to": "none",
+    }
+    defaults.update(overrides)
+    return BaseGLiNER.create_training_args(**defaults)
+
+
+def _get_train_model_signature():
+    """Return the signature of BaseGLiNER.train_model."""
+    from gliner.model import BaseGLiNER
+
+    return inspect.signature(BaseGLiNER.train_model)
+
+
+def _get_train_model_source():
+    """Return the source code of BaseGLiNER.train_model."""
+    from gliner.model import BaseGLiNER
+
+    return inspect.getsource(BaseGLiNER.train_model)
+
+
+# ===========================================================================
+# 1. Smoke tests  --  these MUST pass
+# ===========================================================================
+
+
+class TestSmoke:
+    """Basic smoke tests for training module imports and instantiation."""
+
+    def test_training_args_import(self):
+        from gliner.training import TrainingArguments as TA
+
+        assert TA is not None
+
+    def test_trainer_import(self):
+        from gliner.training import Trainer as T
+
+        assert T is not None
+
+    def test_training_args_instantiation(self):
+        args = _make_training_args()
+        assert args.output_dir == "/tmp/gliner_test_output"
+
+    def test_create_training_args_callable(self):
+        from gliner.model import BaseGLiNER
+
+        assert callable(BaseGLiNER.create_training_args)
+
+    def test_create_training_args_returns_training_args(self):
+        args = _create_training_args_via_classmethod()
+        assert isinstance(args, TrainingArguments)
+
+
+# ===========================================================================
+# 2. TrainingArguments defaults  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainingArgumentsDefaults:
+    """Verify TrainingArguments field defaults match expected values."""
+
+    def test_masking_default_is_global(self):
+        """TrainingArguments.masking should default to 'global'."""
+        args = _make_training_args()
+        assert args.masking == "global"
+
+    def test_label_smoothing_field_exists(self):
+        field_names = _get_training_args_field_names()
+        assert "label_smoothing" in field_names
+
+    def test_loss_reduction_default_is_sum(self):
+        args = _make_training_args()
+        assert args.loss_reduction == "sum"
+
+    def test_focal_loss_defaults(self):
+        args = _make_training_args()
+        assert args.focal_loss_alpha == -1
+        assert args.focal_loss_gamma == 0
+        assert args.focal_loss_prob_margin == 0
+
+
+# ===========================================================================
+# 3. TrainingArguments custom fields completeness  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainingArgumentsFieldCompleteness:
+    """Verify all GLiNER-specific fields are properly declared."""
+
+    REQUIRED_GLINER_FIELDS = {
+        "focal_loss_alpha",
+        "focal_loss_gamma",
+        "focal_loss_prob_margin",
+        "label_smoothing",
+        "loss_reduction",
+        "negatives",
+        "masking",
+        "others_lr",
+        "others_weight_decay",
+    }
+
+    def test_all_gliner_fields_declared(self):
+        field_names = _get_training_args_field_names()
+        missing = self.REQUIRED_GLINER_FIELDS - field_names
+        assert not missing, f"Missing GLiNER-specific fields: {missing}"
+
+    def test_gliner_fields_have_defaults(self):
+        """All GLiNER-specific fields should have defaults (instantiation with only output_dir)."""
+        try:
+            _make_training_args()
+        except TypeError as e:
+            pytest.fail(f"TrainingArguments cannot be instantiated with defaults only: {e}")
+
+
+# ===========================================================================
+# 4. Compute loss wiring  --  this MUST pass
+# ===========================================================================
+
+
+class TestComputeLossWiring:
+    """Verify Trainer.compute_loss correctly forwards all loss parameters."""
+
+    def test_compute_loss_passes_all_loss_params(self):
+        source = inspect.getsource(Trainer.compute_loss)
+        required_forwards = [
+            ("alpha", "self.args.focal_loss_alpha"),
+            ("gamma", "self.args.focal_loss_gamma"),
+            ("prob_margin", "self.args.focal_loss_prob_margin"),
+            ("label_smoothing", "self.args.label_smoothing"),
+            ("reduction", "self.args.loss_reduction"),
+            ("negatives", "self.args.negatives"),
+            ("masking", "self.args.masking"),
+        ]
+        missing = []
+        for param_name, attr_path in required_forwards:
+            if attr_path not in source:
+                missing.append(f"{param_name} (via {attr_path})")
+        assert not missing, f"compute_loss does not forward: {', '.join(missing)}"
+
+
+# ===========================================================================
+# 5. Trainer dataloader wiring  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainerDataloaderWiring:
+    """Verify the Trainer correctly uses dataloader configuration from args."""
+
+    def test_get_train_dataloader_uses_pin_memory(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_pin_memory" in source
+
+    def test_get_train_dataloader_uses_persistent_workers(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_persistent_workers" in source
+
+    def test_get_train_dataloader_uses_prefetch_factor(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_prefetch_factor" in source
+
+
+# ===========================================================================
+# 6. kwargs pass-through integrity  --  these MUST pass
+# ===========================================================================
+
+
+class TestKwargsPassThrough:
+    """Verify kwargs passed to create_training_args reach TrainingArguments."""
+
+    def test_label_smoothing_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(label_smoothing=0.1)
+        assert args.label_smoothing == 0.1
+
+    def test_fp16_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(fp16=True)
+        assert args.fp16 is True
+
+    def test_seed_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(seed=42)
+        assert args.seed == 42
+
+
+# ===========================================================================
+# 7. HF default for remove_unused_columns  --  documents the risk
+# ===========================================================================
+
+
+class TestHFDefaults:
+    """Document HF default values that create risk for GLiNER."""
+
+    def test_hf_remove_unused_columns_defaults_to_true(self):
+        """HF Trainer defaults remove_unused_columns to True.
+
+        This is dangerous for GLiNER which uses custom batch dictionaries.
+        """
+        hf_args = transformers.TrainingArguments(
+            output_dir="/tmp/test", report_to="none"
+        )
+        assert hf_args.remove_unused_columns is True
+
+    def test_hf_label_smoothing_factor_defaults_to_zero(self):
+        """HF label_smoothing_factor defaults to 0 (no double smoothing by default)."""
+        args = _make_training_args(label_smoothing=0.1)
+        hf_ls = getattr(args, "label_smoothing_factor", 0)
+        assert hf_ls == 0
+
+
+# ===========================================================================
+#
+#   BUG-CATCHING TESTS  --  xfail(strict=True)
+#
+#   These tests assert the CORRECT behavior. They currently fail because the
+#   code has known issues. When an issue is fixed, the test will XPASS and
+#   the marker should be removed.
+#
+# ===========================================================================
+
+
+# ---------------------------------------------------------------------------
+# 8. create_training_args explicit parameter coverage gaps
+# ---------------------------------------------------------------------------
+
+
+class TestCreateTrainingArgsSignature:
+    """Verify create_training_args has explicit parameters for critical fields.
+
+    Fields relying on **kwargs are fragile: not discoverable, not documented
+    in the signature, and can silently break if TrainingArguments changes.
+    """
+
+    @pytest.fixture(scope="class")
+    def explicit_params(self):
+        return _get_create_training_args_explicit_params()
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="label_smoothing relies on **kwargs; not explicit in create_training_args",
+    )
+    def test_label_smoothing_is_explicit(self, explicit_params):
+        assert "label_smoothing" in explicit_params, (
+            "label_smoothing is not an explicit parameter in create_training_args; "
+            "it relies on **kwargs pass-through which is fragile"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="fp16 is not explicit; only bf16 is, creating an asymmetry",
+    )
+    def test_fp16_is_explicit(self, explicit_params):
+        assert "fp16" in explicit_params, (
+            "fp16 is not explicit in create_training_args; only bf16 is"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="seed not explicit; Trainer's internal seed may differ from user's",
+    )
+    def test_seed_is_explicit(self, explicit_params):
+        assert "seed" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="gradient_checkpointing not explicit; critical for large models",
+    )
+    def test_gradient_checkpointing_is_explicit(self, explicit_params):
+        assert "gradient_checkpointing" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="run_name not explicit; experiment trackers get auto-generated names",
+    )
+    def test_run_name_is_explicit(self, explicit_params):
+        assert "run_name" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="push_to_hub not explicit in create_training_args",
+    )
+    def test_push_to_hub_is_explicit(self, explicit_params):
+        assert "push_to_hub" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="hub_model_id not explicit in create_training_args",
+    )
+    def test_hub_model_id_is_explicit(self, explicit_params):
+        assert "hub_model_id" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="evaluation_strategy/eval_strategy not explicit; evaluation never runs",
+    )
+    def test_evaluation_strategy_is_explicit(self, explicit_params):
+        has_eval = (
+            "evaluation_strategy" in explicit_params
+            or "eval_strategy" in explicit_params
+        )
+        assert has_eval, (
+            "Neither evaluation_strategy nor eval_strategy is explicit; "
+            "evaluation never runs during training"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="eval_steps not explicit; evaluation frequency cannot be controlled",
+    )
+    def test_eval_steps_is_explicit(self, explicit_params):
+        assert "eval_steps" in explicit_params
+
+
+# ---------------------------------------------------------------------------
+# 9. Masking default mismatch
+# ---------------------------------------------------------------------------
+
+
+class TestMaskingDefaultMismatch:
+    """Detect default mismatch between create_training_args and TrainingArguments."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "create_training_args defaults masking to 'none' but "
+            "TrainingArguments defaults to 'global'"
+        ),
+    )
+    def test_create_training_args_masking_matches_training_args_default(self):
+        """The two defaults should agree so create_training_args doesn't
+        silently override the GLiNER default.
+        """
+        sig = inspect.signature(_create_training_args_via_classmethod)
+        # Actually inspect the real method
+        from gliner.model import BaseGLiNER
+
+        real_sig = inspect.signature(BaseGLiNER.create_training_args)
+        cta_default = real_sig.parameters["masking"].default
+
+        ta_fields = {f.name: f for f in dataclass_fields(TrainingArguments)}
+        ta_default = ta_fields["masking"].default
+
+        assert cta_default == ta_default, (
+            f"Masking default mismatch: create_training_args='{cta_default}' "
+            f"vs TrainingArguments='{ta_default}'"
+        )
+
+
+# ---------------------------------------------------------------------------
+# 10. remove_unused_columns not set to False
+# ---------------------------------------------------------------------------
+
+
+class TestRemoveUnusedColumns:
+    """GLiNER uses custom batch dicts that require remove_unused_columns=False."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "create_training_args does not set remove_unused_columns=False; "
+            "HF Trainer defaults to True which silently drops custom batch keys"
+        ),
+    )
+    def test_create_training_args_sets_remove_unused_columns_false(self):
+        args = _create_training_args_via_classmethod()
+        assert args.remove_unused_columns is False, (
+            f"remove_unused_columns is {args.remove_unused_columns}. "
+            f"GLiNER needs False to preserve custom batch dictionary keys."
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "train_model never references remove_unused_columns; "
+            "GLiNER's custom batch dictionaries require it to be False"
+        ),
+    )
+    def test_train_model_source_references_remove_unused_columns(self):
+        source = _get_train_model_source()
+        assert "remove_unused_columns" in source, (
+            "train_model never references remove_unused_columns. "
+            "HF defaults to True which can cause silent data loss."
+        )
+
+
+# ---------------------------------------------------------------------------
+# 11. Evaluation strategy not configured
+# ---------------------------------------------------------------------------
+
+
+class TestEvaluationConfiguration:
+    """Verify that evaluation is properly configured during training."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "create_training_args never sets evaluation_strategy='steps'; "
+            "evaluation never runs during training"
+        ),
+    )
+    def test_create_training_args_enables_evaluation(self):
+        args = _create_training_args_via_classmethod(save_steps=500)
+        eval_strategy = getattr(args, "eval_strategy", None) or getattr(
+            args, "evaluation_strategy", None
+        )
+        assert eval_strategy == "steps", (
+            f"evaluation_strategy is '{eval_strategy}', not 'steps'. "
+            f"Evaluation never runs."
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="eval_steps is never set by create_training_args",
+    )
+    def test_create_training_args_forwards_eval_steps(self):
+        args = _create_training_args_via_classmethod(save_steps=500)
+        eval_steps = getattr(args, "eval_steps", None)
+        assert eval_steps is not None and eval_steps > 0, (
+            f"eval_steps is {eval_steps}; evaluation won't run at a meaningful frequency"
+        )
+
+
+# ---------------------------------------------------------------------------
+# 12. Dead config fields (size_sup, shuffle_types, random_drop)
+# ---------------------------------------------------------------------------
+
+
+class TestDeadConfigFields:
+    """Config fields that exist but are never consumed by any code."""
+
+    @pytest.fixture(scope="class")
+    def gliner_source_dir(self):
+        return Path(__file__).parent.parent / "gliner"
+
+    def _field_consumed_in_source(self, field_name, source_dir):
+        for py_file in source_dir.rglob("*.py"):
+            try:
+                content = py_file.read_text()
+            except (OSError, UnicodeDecodeError):
+                continue
+            if field_name in content:
+                return True
+        return False
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="size_sup is in config.yaml but never consumed by gliner/",
+    )
+    def test_size_sup_is_consumed(self, gliner_source_dir):
+        assert self._field_consumed_in_source("size_sup", gliner_source_dir)
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="shuffle_types is in config.yaml but never consumed by gliner/",
+    )
+    def test_shuffle_types_is_consumed(self, gliner_source_dir):
+        assert self._field_consumed_in_source("shuffle_types", gliner_source_dir)
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="random_drop is in config.yaml but never consumed by gliner/",
+    )
+    def test_random_drop_is_consumed(self, gliner_source_dir):
+        assert self._field_consumed_in_source("random_drop", gliner_source_dir)
+
+
+# ---------------------------------------------------------------------------
+# 13. label_smoothing name collision
+# ---------------------------------------------------------------------------
+
+
+class TestLabelSmoothingCollision:
+    """GLiNER's label_smoothing and HF's label_smoothing_factor can collide."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "TrainingArguments inherits label_smoothing_factor from HF AND "
+            "adds label_smoothing; both can be active simultaneously"
+        ),
+    )
+    def test_no_dual_label_smoothing_fields(self):
+        """TrainingArguments should not have BOTH label_smoothing AND
+        label_smoothing_factor, since they represent two different smoothing
+        mechanisms that can accidentally combine.
+        """
+        args = _make_training_args()
+        has_gliner_ls = hasattr(args, "label_smoothing")
+        has_hf_ls = hasattr(args, "label_smoothing_factor")
+        assert not (has_gliner_ls and has_hf_ls), (
+            "TrainingArguments has BOTH 'label_smoothing' (GLiNER, applied "
+            "in model forward) AND 'label_smoothing_factor' (HF, applied in "
+            "Trainer.compute_loss). Double smoothing risk."
+        )
+
+
+# ---------------------------------------------------------------------------
+# 14. train_model does not support resume_from_checkpoint
+# ---------------------------------------------------------------------------
+
+
+class TestTrainModelResumeSupport:
+    """train_model should support checkpoint resumption."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="train_model does not accept resume_from_checkpoint parameter",
+    )
+    def test_train_model_accepts_resume_from_checkpoint(self):
+        sig = _get_train_model_signature()
+        param_names = set(sig.parameters.keys())
+        assert "resume_from_checkpoint" in param_names
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="train_model source never references resume_from_checkpoint",
+    )
+    def test_train_model_forwards_resume_to_trainer(self):
+        source = _get_train_model_source()
+        assert "resume_from_checkpoint" in source
+
+
+# ---------------------------------------------------------------------------
+# 15. create_training_args should expose all custom TrainingArguments fields
+# ---------------------------------------------------------------------------
+
+
+class TestCreateTrainingArgsCoversCustomFields:
+    """Every custom field on TrainingArguments should be explicit in
+    create_training_args, not buried in **kwargs.
+    """
+
+    CUSTOM_FIELDS = {
+        "others_lr",
+        "others_weight_decay",
+        "focal_loss_alpha",
+        "focal_loss_gamma",
+        "focal_loss_prob_margin",
+        "label_smoothing",
+        "loss_reduction",
+        "negatives",
+        "masking",
+    }
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="label_smoothing (at minimum) is not explicit in create_training_args",
+    )
+    def test_all_custom_fields_are_explicit_params(self):
+        explicit = _get_create_training_args_explicit_params()
+        missing = self.CUSTOM_FIELDS - explicit
+        assert not missing, (
+            f"These custom TrainingArguments fields are not explicit parameters "
+            f"in create_training_args: {missing}. They rely on **kwargs."
+        )
+
+
+# ---------------------------------------------------------------------------
+# 16. Config YAML dead field detection (aggregate)
+# ---------------------------------------------------------------------------
+
+
+class TestConfigYamlDeadFields:
+    """Verify all training fields in config.yaml are consumed by code."""
+
+    @pytest.fixture(scope="class")
+    def config_yaml_path(self):
+        return Path(__file__).parent.parent / "configs" / "config.yaml"
+
+    @pytest.fixture(scope="class")
+    def config_training_fields(self, config_yaml_path):
+        if not config_yaml_path.exists():
+            pytest.skip("configs/config.yaml not found")
+        fields = []
+        in_training = False
+        content = config_yaml_path.read_text()
+        for line in content.splitlines():
+            stripped = line.strip()
+            if stripped.startswith("#") or not stripped:
+                continue
+            if not line.startswith(" ") and not line.startswith("\t") and ":" in stripped:
+                section = stripped.split(":")[0].strip()
+                in_training = section == "training"
+                continue
+            if in_training and ":" in stripped:
+                field_name = stripped.split(":")[0].strip()
+                if field_name and not field_name.startswith("#"):
+                    fields.append(field_name)
+        return fields
```
</details>

![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

This fixture manually parses the `config.yaml` file. This implementation is fragile as it relies on specific line formatting (e.g., indentation, no comments on the same line as a section header) and could break with small changes to the YAML file.

Using a dedicated YAML parsing library like `PyYAML` would make this much more robust and simpler. You can add `pyyaml` to your test dependencies and `import yaml` at the top of the file.

```python
    def config_training_fields(self, config_yaml_path):
        if not config_yaml_path.exists():
            pytest.skip("configs/config.yaml not found")
        with config_yaml_path.open("r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        training_config = config.get("training", {})
        return list(training_config.keys()) if training_config else []
```

---

### gemini-code-assist[bot] &mdash; 2/17/2026, 7:11:57 PM

> File: `tests/test_training_validation.py`

<details><summary>Code context</summary>

```diff
@@ -0,0 +1,684 @@
+"""Tests for training parameter validation and configuration wiring.
+
+These tests verify that training parameters are correctly defined, forwarded,
+and defaulted throughout the GLiNER training pipeline. They catch issues
+identified in the training parameters and loss configuration report.
+
+Tests marked with ``@pytest.mark.xfail(strict=True)`` document known gaps.
+When a gap is fixed the test will unexpectedly pass (XPASS) and the marker
+should be removed.
+
+Test categories:
+    - TrainingArguments field definitions and defaults
+    - create_training_args explicit parameter coverage
+    - train_model parameter forwarding
+    - Config field consumption (dead fields)
+    - Label smoothing name collision risk
+"""
+
+import inspect
+from dataclasses import fields as dataclass_fields
+from pathlib import Path
+
+import pytest
+import transformers
+
+from gliner.training.trainer import TrainingArguments, Trainer
+
+
+# ---------------------------------------------------------------------------
+# Helpers
+# ---------------------------------------------------------------------------
+
+
+def _get_create_training_args_explicit_params():
+    """Return the set of explicit parameter names in create_training_args.
+
+    Imports BaseGLiNER and inspects the classmethod signature, excluding
+    'cls', 'self', and '**kwargs'.
+    """
+    from gliner.model import BaseGLiNER
+
+    sig = inspect.signature(BaseGLiNER.create_training_args)
+    return {
+        name
+        for name, param in sig.parameters.items()
+        if name not in ("cls", "self")
+        and param.kind
+        not in (
+            inspect.Parameter.VAR_KEYWORD,
+            inspect.Parameter.VAR_POSITIONAL,
+        )
+    }
+
+
+def _get_training_args_field_names():
+    """Return all field names defined on the custom TrainingArguments dataclass."""
+    return {f.name for f in dataclass_fields(TrainingArguments)}
+
+
+def _make_training_args(**overrides):
+    """Create a TrainingArguments with a temporary output_dir and no reporting."""
+    defaults = {
+        "output_dir": "/tmp/gliner_test_output",
+        "report_to": "none",
+        "use_cpu": True,
+    }
+    defaults.update(overrides)
+    return TrainingArguments(**defaults)
+
+
+def _create_training_args_via_classmethod(**overrides):
+    """Create TrainingArguments through BaseGLiNER.create_training_args."""
+    from gliner.model import BaseGLiNER
+
+    defaults = {
+        "output_dir": "/tmp/gliner_test_output",
+        "report_to": "none",
+    }
+    defaults.update(overrides)
+    return BaseGLiNER.create_training_args(**defaults)
+
+
+def _get_train_model_signature():
+    """Return the signature of BaseGLiNER.train_model."""
+    from gliner.model import BaseGLiNER
+
+    return inspect.signature(BaseGLiNER.train_model)
+
+
+def _get_train_model_source():
+    """Return the source code of BaseGLiNER.train_model."""
+    from gliner.model import BaseGLiNER
+
+    return inspect.getsource(BaseGLiNER.train_model)
+
+
+# ===========================================================================
+# 1. Smoke tests  --  these MUST pass
+# ===========================================================================
+
+
+class TestSmoke:
+    """Basic smoke tests for training module imports and instantiation."""
+
+    def test_training_args_import(self):
+        from gliner.training import TrainingArguments as TA
+
+        assert TA is not None
+
+    def test_trainer_import(self):
+        from gliner.training import Trainer as T
+
+        assert T is not None
+
+    def test_training_args_instantiation(self):
+        args = _make_training_args()
+        assert args.output_dir == "/tmp/gliner_test_output"
+
+    def test_create_training_args_callable(self):
+        from gliner.model import BaseGLiNER
+
+        assert callable(BaseGLiNER.create_training_args)
+
+    def test_create_training_args_returns_training_args(self):
+        args = _create_training_args_via_classmethod()
+        assert isinstance(args, TrainingArguments)
+
+
+# ===========================================================================
+# 2. TrainingArguments defaults  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainingArgumentsDefaults:
+    """Verify TrainingArguments field defaults match expected values."""
+
+    def test_masking_default_is_global(self):
+        """TrainingArguments.masking should default to 'global'."""
+        args = _make_training_args()
+        assert args.masking == "global"
+
+    def test_label_smoothing_field_exists(self):
+        field_names = _get_training_args_field_names()
+        assert "label_smoothing" in field_names
+
+    def test_loss_reduction_default_is_sum(self):
+        args = _make_training_args()
+        assert args.loss_reduction == "sum"
+
+    def test_focal_loss_defaults(self):
+        args = _make_training_args()
+        assert args.focal_loss_alpha == -1
+        assert args.focal_loss_gamma == 0
+        assert args.focal_loss_prob_margin == 0
+
+
+# ===========================================================================
+# 3. TrainingArguments custom fields completeness  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainingArgumentsFieldCompleteness:
+    """Verify all GLiNER-specific fields are properly declared."""
+
+    REQUIRED_GLINER_FIELDS = {
+        "focal_loss_alpha",
+        "focal_loss_gamma",
+        "focal_loss_prob_margin",
+        "label_smoothing",
+        "loss_reduction",
+        "negatives",
+        "masking",
+        "others_lr",
+        "others_weight_decay",
+    }
+
+    def test_all_gliner_fields_declared(self):
+        field_names = _get_training_args_field_names()
+        missing = self.REQUIRED_GLINER_FIELDS - field_names
+        assert not missing, f"Missing GLiNER-specific fields: {missing}"
+
+    def test_gliner_fields_have_defaults(self):
+        """All GLiNER-specific fields should have defaults (instantiation with only output_dir)."""
+        try:
+            _make_training_args()
+        except TypeError as e:
+            pytest.fail(f"TrainingArguments cannot be instantiated with defaults only: {e}")
+
+
+# ===========================================================================
+# 4. Compute loss wiring  --  this MUST pass
+# ===========================================================================
+
+
+class TestComputeLossWiring:
+    """Verify Trainer.compute_loss correctly forwards all loss parameters."""
+
+    def test_compute_loss_passes_all_loss_params(self):
+        source = inspect.getsource(Trainer.compute_loss)
+        required_forwards = [
+            ("alpha", "self.args.focal_loss_alpha"),
+            ("gamma", "self.args.focal_loss_gamma"),
+            ("prob_margin", "self.args.focal_loss_prob_margin"),
+            ("label_smoothing", "self.args.label_smoothing"),
+            ("reduction", "self.args.loss_reduction"),
+            ("negatives", "self.args.negatives"),
+            ("masking", "self.args.masking"),
+        ]
+        missing = []
+        for param_name, attr_path in required_forwards:
+            if attr_path not in source:
+                missing.append(f"{param_name} (via {attr_path})")
+        assert not missing, f"compute_loss does not forward: {', '.join(missing)}"
+
+
+# ===========================================================================
+# 5. Trainer dataloader wiring  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainerDataloaderWiring:
+    """Verify the Trainer correctly uses dataloader configuration from args."""
+
+    def test_get_train_dataloader_uses_pin_memory(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_pin_memory" in source
+
+    def test_get_train_dataloader_uses_persistent_workers(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_persistent_workers" in source
+
+    def test_get_train_dataloader_uses_prefetch_factor(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_prefetch_factor" in source
+
+
+# ===========================================================================
+# 6. kwargs pass-through integrity  --  these MUST pass
+# ===========================================================================
+
+
+class TestKwargsPassThrough:
+    """Verify kwargs passed to create_training_args reach TrainingArguments."""
+
+    def test_label_smoothing_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(label_smoothing=0.1)
+        assert args.label_smoothing == 0.1
+
+    def test_fp16_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(fp16=True)
+        assert args.fp16 is True
+
+    def test_seed_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(seed=42)
+        assert args.seed == 42
+
+
+# ===========================================================================
+# 7. HF default for remove_unused_columns  --  documents the risk
+# ===========================================================================
+
+
+class TestHFDefaults:
+    """Document HF default values that create risk for GLiNER."""
+
+    def test_hf_remove_unused_columns_defaults_to_true(self):
+        """HF Trainer defaults remove_unused_columns to True.
+
+        This is dangerous for GLiNER which uses custom batch dictionaries.
+        """
+        hf_args = transformers.TrainingArguments(
+            output_dir="/tmp/test", report_to="none"
+        )
+        assert hf_args.remove_unused_columns is True
+
+    def test_hf_label_smoothing_factor_defaults_to_zero(self):
+        """HF label_smoothing_factor defaults to 0 (no double smoothing by default)."""
+        args = _make_training_args(label_smoothing=0.1)
+        hf_ls = getattr(args, "label_smoothing_factor", 0)
+        assert hf_ls == 0
+
+
+# ===========================================================================
+#
+#   BUG-CATCHING TESTS  --  xfail(strict=True)
+#
+#   These tests assert the CORRECT behavior. They currently fail because the
+#   code has known issues. When an issue is fixed, the test will XPASS and
+#   the marker should be removed.
+#
+# ===========================================================================
+
+
+# ---------------------------------------------------------------------------
+# 8. create_training_args explicit parameter coverage gaps
+# ---------------------------------------------------------------------------
+
+
+class TestCreateTrainingArgsSignature:
+    """Verify create_training_args has explicit parameters for critical fields.
+
+    Fields relying on **kwargs are fragile: not discoverable, not documented
+    in the signature, and can silently break if TrainingArguments changes.
+    """
+
+    @pytest.fixture(scope="class")
+    def explicit_params(self):
+        return _get_create_training_args_explicit_params()
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="label_smoothing relies on **kwargs; not explicit in create_training_args",
+    )
+    def test_label_smoothing_is_explicit(self, explicit_params):
+        assert "label_smoothing" in explicit_params, (
+            "label_smoothing is not an explicit parameter in create_training_args; "
+            "it relies on **kwargs pass-through which is fragile"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="fp16 is not explicit; only bf16 is, creating an asymmetry",
+    )
+    def test_fp16_is_explicit(self, explicit_params):
+        assert "fp16" in explicit_params, (
+            "fp16 is not explicit in create_training_args; only bf16 is"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="seed not explicit; Trainer's internal seed may differ from user's",
+    )
+    def test_seed_is_explicit(self, explicit_params):
+        assert "seed" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="gradient_checkpointing not explicit; critical for large models",
+    )
+    def test_gradient_checkpointing_is_explicit(self, explicit_params):
+        assert "gradient_checkpointing" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="run_name not explicit; experiment trackers get auto-generated names",
+    )
+    def test_run_name_is_explicit(self, explicit_params):
+        assert "run_name" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="push_to_hub not explicit in create_training_args",
+    )
+    def test_push_to_hub_is_explicit(self, explicit_params):
+        assert "push_to_hub" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="hub_model_id not explicit in create_training_args",
+    )
+    def test_hub_model_id_is_explicit(self, explicit_params):
+        assert "hub_model_id" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="evaluation_strategy/eval_strategy not explicit; evaluation never runs",
+    )
+    def test_evaluation_strategy_is_explicit(self, explicit_params):
+        has_eval = (
+            "evaluation_strategy" in explicit_params
+            or "eval_strategy" in explicit_params
+        )
+        assert has_eval, (
+            "Neither evaluation_strategy nor eval_strategy is explicit; "
+            "evaluation never runs during training"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="eval_steps not explicit; evaluation frequency cannot be controlled",
+    )
+    def test_eval_steps_is_explicit(self, explicit_params):
+        assert "eval_steps" in explicit_params
+
+
+# ---------------------------------------------------------------------------
+# 9. Masking default mismatch
+# ---------------------------------------------------------------------------
+
+
+class TestMaskingDefaultMismatch:
+    """Detect default mismatch between create_training_args and TrainingArguments."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "create_training_args defaults masking to 'none' but "
+            "TrainingArguments defaults to 'global'"
+        ),
+    )
+    def test_create_training_args_masking_matches_training_args_default(self):
+        """The two defaults should agree so create_training_args doesn't
+        silently override the GLiNER default.
+        """
+        sig = inspect.signature(_create_training_args_via_classmethod)
+        # Actually inspect the real method
+        from gliner.model import BaseGLiNER
+
+        real_sig = inspect.signature(BaseGLiNER.create_training_args)
+        cta_default = real_sig.parameters["masking"].default
+
+        ta_fields = {f.name: f for f in dataclass_fields(TrainingArguments)}
+        ta_default = ta_fields["masking"].default
+
+        assert cta_default == ta_default, (
+            f"Masking default mismatch: create_training_args='{cta_default}' "
+            f"vs TrainingArguments='{ta_default}'"
+        )
+
+
+# ---------------------------------------------------------------------------
+# 10. remove_unused_columns not set to False
+# ---------------------------------------------------------------------------
+
+
+class TestRemoveUnusedColumns:
+    """GLiNER uses custom batch dicts that require remove_unused_columns=False."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "create_training_args does not set remove_unused_columns=False; "
+            "HF Trainer defaults to True which silently drops custom batch keys"
+        ),
+    )
+    def test_create_training_args_sets_remove_unused_columns_false(self):
+        args = _create_training_args_via_classmethod()
+        assert args.remove_unused_columns is False, (
+            f"remove_unused_columns is {args.remove_unused_columns}. "
+            f"GLiNER needs False to preserve custom batch dictionary keys."
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "train_model never references remove_unused_columns; "
+            "GLiNER's custom batch dictionaries require it to be False"
+        ),
+    )
+    def test_train_model_source_references_remove_unused_columns(self):
+        source = _get_train_model_source()
+        assert "remove_unused_columns" in source, (
+            "train_model never references remove_unused_columns. "
+            "HF defaults to True which can cause silent data loss."
+        )
+
+
+# ---------------------------------------------------------------------------
+# 11. Evaluation strategy not configured
+# ---------------------------------------------------------------------------
+
+
+class TestEvaluationConfiguration:
+    """Verify that evaluation is properly configured during training."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "create_training_args never sets evaluation_strategy='steps'; "
+            "evaluation never runs during training"
+        ),
+    )
+    def test_create_training_args_enables_evaluation(self):
+        args = _create_training_args_via_classmethod(save_steps=500)
+        eval_strategy = getattr(args, "eval_strategy", None) or getattr(
+            args, "evaluation_strategy", None
+        )
+        assert eval_strategy == "steps", (
+            f"evaluation_strategy is '{eval_strategy}', not 'steps'. "
+            f"Evaluation never runs."
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="eval_steps is never set by create_training_args",
+    )
+    def test_create_training_args_forwards_eval_steps(self):
+        args = _create_training_args_via_classmethod(save_steps=500)
+        eval_steps = getattr(args, "eval_steps", None)
+        assert eval_steps is not None and eval_steps > 0, (
+            f"eval_steps is {eval_steps}; evaluation won't run at a meaningful frequency"
+        )
+
+
+# ---------------------------------------------------------------------------
+# 12. Dead config fields (size_sup, shuffle_types, random_drop)
+# ---------------------------------------------------------------------------
+
+
+class TestDeadConfigFields:
+    """Config fields that exist but are never consumed by any code."""
+
+    @pytest.fixture(scope="class")
+    def gliner_source_dir(self):
+        return Path(__file__).parent.parent / "gliner"
+
+    def _field_consumed_in_source(self, field_name, source_dir):
+        for py_file in source_dir.rglob("*.py"):
+            try:
+                content = py_file.read_text()
+            except (OSError, UnicodeDecodeError):
+                continue
+            if field_name in content:
+                return True
+        return False
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="size_sup is in config.yaml but never consumed by gliner/",
+    )
+    def test_size_sup_is_consumed(self, gliner_source_dir):
+        assert self._field_consumed_in_source("size_sup", gliner_source_dir)
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="shuffle_types is in config.yaml but never consumed by gliner/",
+    )
+    def test_shuffle_types_is_consumed(self, gliner_source_dir):
+        assert self._field_consumed_in_source("shuffle_types", gliner_source_dir)
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="random_drop is in config.yaml but never consumed by gliner/",
+    )
+    def test_random_drop_is_consumed(self, gliner_source_dir):
+        assert self._field_consumed_in_source("random_drop", gliner_source_dir)
+
+
+# ---------------------------------------------------------------------------
+# 13. label_smoothing name collision
+# ---------------------------------------------------------------------------
+
+
+class TestLabelSmoothingCollision:
+    """GLiNER's label_smoothing and HF's label_smoothing_factor can collide."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "TrainingArguments inherits label_smoothing_factor from HF AND "
+            "adds label_smoothing; both can be active simultaneously"
+        ),
+    )
+    def test_no_dual_label_smoothing_fields(self):
+        """TrainingArguments should not have BOTH label_smoothing AND
+        label_smoothing_factor, since they represent two different smoothing
+        mechanisms that can accidentally combine.
+        """
+        args = _make_training_args()
+        has_gliner_ls = hasattr(args, "label_smoothing")
+        has_hf_ls = hasattr(args, "label_smoothing_factor")
+        assert not (has_gliner_ls and has_hf_ls), (
+            "TrainingArguments has BOTH 'label_smoothing' (GLiNER, applied "
+            "in model forward) AND 'label_smoothing_factor' (HF, applied in "
+            "Trainer.compute_loss). Double smoothing risk."
+        )
+
+
+# ---------------------------------------------------------------------------
+# 14. train_model does not support resume_from_checkpoint
+# ---------------------------------------------------------------------------
+
+
+class TestTrainModelResumeSupport:
+    """train_model should support checkpoint resumption."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="train_model does not accept resume_from_checkpoint parameter",
+    )
+    def test_train_model_accepts_resume_from_checkpoint(self):
+        sig = _get_train_model_signature()
+        param_names = set(sig.parameters.keys())
+        assert "resume_from_checkpoint" in param_names
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="train_model source never references resume_from_checkpoint",
+    )
+    def test_train_model_forwards_resume_to_trainer(self):
+        source = _get_train_model_source()
+        assert "resume_from_checkpoint" in source
+
+
+# ---------------------------------------------------------------------------
+# 15. create_training_args should expose all custom TrainingArguments fields
+# ---------------------------------------------------------------------------
+
+
+class TestCreateTrainingArgsCoversCustomFields:
+    """Every custom field on TrainingArguments should be explicit in
+    create_training_args, not buried in **kwargs.
+    """
+
+    CUSTOM_FIELDS = {
+        "others_lr",
+        "others_weight_decay",
+        "focal_loss_alpha",
+        "focal_loss_gamma",
+        "focal_loss_prob_margin",
+        "label_smoothing",
+        "loss_reduction",
+        "negatives",
+        "masking",
+    }
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="label_smoothing (at minimum) is not explicit in create_training_args",
+    )
+    def test_all_custom_fields_are_explicit_params(self):
+        explicit = _get_create_training_args_explicit_params()
+        missing = self.CUSTOM_FIELDS - explicit
+        assert not missing, (
+            f"These custom TrainingArguments fields are not explicit parameters "
+            f"in create_training_args: {missing}. They rely on **kwargs."
+        )
+
+
+# ---------------------------------------------------------------------------
+# 16. Config YAML dead field detection (aggregate)
+# ---------------------------------------------------------------------------
+
+
+class TestConfigYamlDeadFields:
+    """Verify all training fields in config.yaml are consumed by code."""
+
+    @pytest.fixture(scope="class")
+    def config_yaml_path(self):
+        return Path(__file__).parent.parent / "configs" / "config.yaml"
+
+    @pytest.fixture(scope="class")
+    def config_training_fields(self, config_yaml_path):
+        if not config_yaml_path.exists():
+            pytest.skip("configs/config.yaml not found")
+        fields = []
+        in_training = False
+        content = config_yaml_path.read_text()
+        for line in content.splitlines():
+            stripped = line.strip()
+            if stripped.startswith("#") or not stripped:
+                continue
+            if not line.startswith(" ") and not line.startswith("\t") and ":" in stripped:
+                section = stripped.split(":")[0].strip()
+                in_training = section == "training"
+                continue
+            if in_training and ":" in stripped:
+                field_name = stripped.split(":")[0].strip()
+                if field_name and not field_name.startswith("#"):
+                    fields.append(field_name)
+        return fields
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="size_sup, shuffle_types, random_drop are dead fields in config.yaml",
+    )
+    def test_all_config_training_fields_have_consumers(self, config_training_fields):
+        gliner_dir = Path(__file__).parent.parent / "gliner"
+        dead_fields = []
+        for field_name in config_training_fields:
+            found = False
+            for py_file in gliner_dir.rglob("*.py"):
+                try:
+                    content = py_file.read_text()
+                except (OSError, UnicodeDecodeError):
+                    continue
+                if field_name in content:
+                    found = True
+                    break
```
</details>

![medium](https://www.gstatic.com/codereviewagent/medium-priority.svg)

The current implementation to check if a config field is consumed uses a simple `in content` string search. This can be unreliable and lead to incorrect test results:
- **False positives**: It could match substrings or occurrences in comments and strings. For example, a field `lr` would also match `others_lr`.
- **False negatives**: It might miss dynamic attribute access like `getattr(config, field_name)`.

A more robust approach would be to use regular expressions with word boundaries (`\b`).

Additionally, this logic is duplicated from the `_field_consumed_in_source` method in `TestDeadConfigFields`. It would be better to extract this into a single, improved helper function.

You'll need to `import re` for the suggestion to work.

```python
            found = False
            pattern = r"\b" + re.escape(field_name) + r"\b"
            for py_file in gliner_dir.rglob("*.py"):
                try:
                    content = py_file.read_text(encoding="utf-8")
                except (OSError, UnicodeDecodeError):
                    continue
                if re.search(pattern, content):
                    found = True
                    break
```

---

### coderabbitai[bot] &mdash; 2/17/2026, 7:08:02 PM

> File: `tests/test_training_validation.py`

<details><summary>Code context</summary>

```diff
@@ -0,0 +1,684 @@
+"""Tests for training parameter validation and configuration wiring.
+
+These tests verify that training parameters are correctly defined, forwarded,
+and defaulted throughout the GLiNER training pipeline. They catch issues
+identified in the training parameters and loss configuration report.
+
+Tests marked with ``@pytest.mark.xfail(strict=True)`` document known gaps.
+When a gap is fixed the test will unexpectedly pass (XPASS) and the marker
+should be removed.
+
+Test categories:
+    - TrainingArguments field definitions and defaults
+    - create_training_args explicit parameter coverage
+    - train_model parameter forwarding
+    - Config field consumption (dead fields)
+    - Label smoothing name collision risk
+"""
+
+import inspect
+from dataclasses import fields as dataclass_fields
+from pathlib import Path
+
+import pytest
+import transformers
+
+from gliner.training.trainer import TrainingArguments, Trainer
+
+
+# ---------------------------------------------------------------------------
+# Helpers
+# ---------------------------------------------------------------------------
+
+
+def _get_create_training_args_explicit_params():
+    """Return the set of explicit parameter names in create_training_args.
+
+    Imports BaseGLiNER and inspects the classmethod signature, excluding
+    'cls', 'self', and '**kwargs'.
+    """
+    from gliner.model import BaseGLiNER
+
+    sig = inspect.signature(BaseGLiNER.create_training_args)
+    return {
+        name
+        for name, param in sig.parameters.items()
+        if name not in ("cls", "self")
+        and param.kind
+        not in (
+            inspect.Parameter.VAR_KEYWORD,
+            inspect.Parameter.VAR_POSITIONAL,
+        )
+    }
+
+
+def _get_training_args_field_names():
+    """Return all field names defined on the custom TrainingArguments dataclass."""
+    return {f.name for f in dataclass_fields(TrainingArguments)}
+
+
+def _make_training_args(**overrides):
+    """Create a TrainingArguments with a temporary output_dir and no reporting."""
+    defaults = {
+        "output_dir": "/tmp/gliner_test_output",
+        "report_to": "none",
+        "use_cpu": True,
+    }
+    defaults.update(overrides)
+    return TrainingArguments(**defaults)
+
+
+def _create_training_args_via_classmethod(**overrides):
+    """Create TrainingArguments through BaseGLiNER.create_training_args."""
+    from gliner.model import BaseGLiNER
+
+    defaults = {
+        "output_dir": "/tmp/gliner_test_output",
+        "report_to": "none",
+    }
+    defaults.update(overrides)
+    return BaseGLiNER.create_training_args(**defaults)
+
+
+def _get_train_model_signature():
+    """Return the signature of BaseGLiNER.train_model."""
+    from gliner.model import BaseGLiNER
+
+    return inspect.signature(BaseGLiNER.train_model)
+
+
+def _get_train_model_source():
+    """Return the source code of BaseGLiNER.train_model."""
+    from gliner.model import BaseGLiNER
+
+    return inspect.getsource(BaseGLiNER.train_model)
+
+
+# ===========================================================================
+# 1. Smoke tests  --  these MUST pass
+# ===========================================================================
+
+
+class TestSmoke:
+    """Basic smoke tests for training module imports and instantiation."""
+
+    def test_training_args_import(self):
+        from gliner.training import TrainingArguments as TA
+
+        assert TA is not None
+
+    def test_trainer_import(self):
+        from gliner.training import Trainer as T
+
+        assert T is not None
+
+    def test_training_args_instantiation(self):
+        args = _make_training_args()
+        assert args.output_dir == "/tmp/gliner_test_output"
+
+    def test_create_training_args_callable(self):
+        from gliner.model import BaseGLiNER
+
+        assert callable(BaseGLiNER.create_training_args)
+
+    def test_create_training_args_returns_training_args(self):
+        args = _create_training_args_via_classmethod()
+        assert isinstance(args, TrainingArguments)
+
+
+# ===========================================================================
+# 2. TrainingArguments defaults  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainingArgumentsDefaults:
+    """Verify TrainingArguments field defaults match expected values."""
+
+    def test_masking_default_is_global(self):
+        """TrainingArguments.masking should default to 'global'."""
+        args = _make_training_args()
+        assert args.masking == "global"
+
+    def test_label_smoothing_field_exists(self):
+        field_names = _get_training_args_field_names()
+        assert "label_smoothing" in field_names
+
+    def test_loss_reduction_default_is_sum(self):
+        args = _make_training_args()
+        assert args.loss_reduction == "sum"
+
+    def test_focal_loss_defaults(self):
+        args = _make_training_args()
+        assert args.focal_loss_alpha == -1
+        assert args.focal_loss_gamma == 0
+        assert args.focal_loss_prob_margin == 0
+
+
+# ===========================================================================
+# 3. TrainingArguments custom fields completeness  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainingArgumentsFieldCompleteness:
+    """Verify all GLiNER-specific fields are properly declared."""
+
+    REQUIRED_GLINER_FIELDS = {
+        "focal_loss_alpha",
+        "focal_loss_gamma",
+        "focal_loss_prob_margin",
+        "label_smoothing",
+        "loss_reduction",
+        "negatives",
+        "masking",
+        "others_lr",
+        "others_weight_decay",
+    }
+
+    def test_all_gliner_fields_declared(self):
+        field_names = _get_training_args_field_names()
+        missing = self.REQUIRED_GLINER_FIELDS - field_names
+        assert not missing, f"Missing GLiNER-specific fields: {missing}"
+
+    def test_gliner_fields_have_defaults(self):
+        """All GLiNER-specific fields should have defaults (instantiation with only output_dir)."""
+        try:
+            _make_training_args()
+        except TypeError as e:
+            pytest.fail(f"TrainingArguments cannot be instantiated with defaults only: {e}")
+
+
+# ===========================================================================
+# 4. Compute loss wiring  --  this MUST pass
+# ===========================================================================
+
+
+class TestComputeLossWiring:
+    """Verify Trainer.compute_loss correctly forwards all loss parameters."""
+
+    def test_compute_loss_passes_all_loss_params(self):
+        source = inspect.getsource(Trainer.compute_loss)
+        required_forwards = [
+            ("alpha", "self.args.focal_loss_alpha"),
+            ("gamma", "self.args.focal_loss_gamma"),
+            ("prob_margin", "self.args.focal_loss_prob_margin"),
+            ("label_smoothing", "self.args.label_smoothing"),
+            ("reduction", "self.args.loss_reduction"),
+            ("negatives", "self.args.negatives"),
+            ("masking", "self.args.masking"),
+        ]
+        missing = []
+        for param_name, attr_path in required_forwards:
+            if attr_path not in source:
+                missing.append(f"{param_name} (via {attr_path})")
+        assert not missing, f"compute_loss does not forward: {', '.join(missing)}"
+
+
+# ===========================================================================
+# 5. Trainer dataloader wiring  --  these MUST pass
+# ===========================================================================
+
+
+class TestTrainerDataloaderWiring:
+    """Verify the Trainer correctly uses dataloader configuration from args."""
+
+    def test_get_train_dataloader_uses_pin_memory(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_pin_memory" in source
+
+    def test_get_train_dataloader_uses_persistent_workers(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_persistent_workers" in source
+
+    def test_get_train_dataloader_uses_prefetch_factor(self):
+        source = inspect.getsource(Trainer.get_train_dataloader)
+        assert "dataloader_prefetch_factor" in source
+
+
+# ===========================================================================
+# 6. kwargs pass-through integrity  --  these MUST pass
+# ===========================================================================
+
+
+class TestKwargsPassThrough:
+    """Verify kwargs passed to create_training_args reach TrainingArguments."""
+
+    def test_label_smoothing_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(label_smoothing=0.1)
+        assert args.label_smoothing == 0.1
+
+    def test_fp16_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(fp16=True)
+        assert args.fp16 is True
+
+    def test_seed_via_kwargs_reaches_training_args(self):
+        args = _create_training_args_via_classmethod(seed=42)
+        assert args.seed == 42
+
+
+# ===========================================================================
+# 7. HF default for remove_unused_columns  --  documents the risk
+# ===========================================================================
+
+
+class TestHFDefaults:
+    """Document HF default values that create risk for GLiNER."""
+
+    def test_hf_remove_unused_columns_defaults_to_true(self):
+        """HF Trainer defaults remove_unused_columns to True.
+
+        This is dangerous for GLiNER which uses custom batch dictionaries.
+        """
+        hf_args = transformers.TrainingArguments(
+            output_dir="/tmp/test", report_to="none"
+        )
+        assert hf_args.remove_unused_columns is True
+
+    def test_hf_label_smoothing_factor_defaults_to_zero(self):
+        """HF label_smoothing_factor defaults to 0 (no double smoothing by default)."""
+        args = _make_training_args(label_smoothing=0.1)
+        hf_ls = getattr(args, "label_smoothing_factor", 0)
+        assert hf_ls == 0
+
+
+# ===========================================================================
+#
+#   BUG-CATCHING TESTS  --  xfail(strict=True)
+#
+#   These tests assert the CORRECT behavior. They currently fail because the
+#   code has known issues. When an issue is fixed, the test will XPASS and
+#   the marker should be removed.
+#
+# ===========================================================================
+
+
+# ---------------------------------------------------------------------------
+# 8. create_training_args explicit parameter coverage gaps
+# ---------------------------------------------------------------------------
+
+
+class TestCreateTrainingArgsSignature:
+    """Verify create_training_args has explicit parameters for critical fields.
+
+    Fields relying on **kwargs are fragile: not discoverable, not documented
+    in the signature, and can silently break if TrainingArguments changes.
+    """
+
+    @pytest.fixture(scope="class")
+    def explicit_params(self):
+        return _get_create_training_args_explicit_params()
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="label_smoothing relies on **kwargs; not explicit in create_training_args",
+    )
+    def test_label_smoothing_is_explicit(self, explicit_params):
+        assert "label_smoothing" in explicit_params, (
+            "label_smoothing is not an explicit parameter in create_training_args; "
+            "it relies on **kwargs pass-through which is fragile"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="fp16 is not explicit; only bf16 is, creating an asymmetry",
+    )
+    def test_fp16_is_explicit(self, explicit_params):
+        assert "fp16" in explicit_params, (
+            "fp16 is not explicit in create_training_args; only bf16 is"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="seed not explicit; Trainer's internal seed may differ from user's",
+    )
+    def test_seed_is_explicit(self, explicit_params):
+        assert "seed" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="gradient_checkpointing not explicit; critical for large models",
+    )
+    def test_gradient_checkpointing_is_explicit(self, explicit_params):
+        assert "gradient_checkpointing" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="run_name not explicit; experiment trackers get auto-generated names",
+    )
+    def test_run_name_is_explicit(self, explicit_params):
+        assert "run_name" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="push_to_hub not explicit in create_training_args",
+    )
+    def test_push_to_hub_is_explicit(self, explicit_params):
+        assert "push_to_hub" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="hub_model_id not explicit in create_training_args",
+    )
+    def test_hub_model_id_is_explicit(self, explicit_params):
+        assert "hub_model_id" in explicit_params
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="evaluation_strategy/eval_strategy not explicit; evaluation never runs",
+    )
+    def test_evaluation_strategy_is_explicit(self, explicit_params):
+        has_eval = (
+            "evaluation_strategy" in explicit_params
+            or "eval_strategy" in explicit_params
+        )
+        assert has_eval, (
+            "Neither evaluation_strategy nor eval_strategy is explicit; "
+            "evaluation never runs during training"
+        )
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason="eval_steps not explicit; evaluation frequency cannot be controlled",
+    )
+    def test_eval_steps_is_explicit(self, explicit_params):
+        assert "eval_steps" in explicit_params
+
+
+# ---------------------------------------------------------------------------
+# 9. Masking default mismatch
+# ---------------------------------------------------------------------------
+
+
+class TestMaskingDefaultMismatch:
+    """Detect default mismatch between create_training_args and TrainingArguments."""
+
+    @pytest.mark.xfail(
+        strict=True,
+        reason=(
+            "create_training_args defaults masking to 'none' but "
+            "TrainingArguments defaults to 'global'"
+        ),
+    )
+    def test_create_training_args_masking_matches_training_args_default(self):
+        """The two defaults should agree so create_training_args doesn't
+        silently override the GLiNER default.
+        """
+        sig = inspect.signature(_create_training_args_via_classmethod)
+        # Actually inspect the real method
+        from gliner.model import BaseGLiNER
+
+        real_sig = inspect.signature(BaseGLiNER.create_training_args)
+        cta_default = real_sig.parameters["masking"].default
+
+        ta_fields = {f.name: f for f in dataclass_fields(TrainingArguments)}
+        ta_default = ta_fields["masking"].default
+
+        assert cta_default == ta_default, (
+            f"Masking default mismatch: create_training_args='{cta_default}' "
+            f"vs TrainingArguments='{ta_default}'"
+        )
+
```
</details>

_ Potential issue_ | _ Minor_

**Remove unused variable `sig`.**

Line 405 assigns `sig` but it's never used. The test correctly uses `real_sig` from `BaseGLiNER.create_training_args` instead.



<details>
<summary> Proposed fix</summary>

```diff
     def test_create_training_args_masking_matches_training_args_default(self):
         """The two defaults should agree so create_training_args doesn't
         silently override the GLiNER default.
         """
-        sig = inspect.signature(_create_training_args_via_classmethod)
-        # Actually inspect the real method
         from gliner.model import BaseGLiNER

         real_sig = inspect.signature(BaseGLiNER.create_training_args)
```
</details>

<!-- suggestion_start -->

<details>
<summary> Committable suggestion</summary>

>  **IMPORTANT**
> Carefully review the code before committing. Ensure that it accurately replaces the highlighted code, contains no missing lines, and has no issues with indentation. Thoroughly test & benchmark the code to ensure it meets the requirements.

```suggestion
    def test_create_training_args_masking_matches_training_args_default(self):
        """The two defaults should agree so create_training_args doesn't
        silently override the GLiNER default.
        """
        from gliner.model import BaseGLiNER

        real_sig = inspect.signature(BaseGLiNER.create_training_args)
        cta_default = real_sig.parameters["masking"].default

        ta_fields = {f.name: f for f in dataclass_fields(TrainingArguments)}
        ta_default = ta_fields["masking"].default

        assert cta_default == ta_default, (
            f"Masking default mismatch: create_training_args='{cta_default}' "
            f"vs TrainingArguments='{ta_default}'"
        )
```

</details>

<!-- suggestion_end -->

<details>
<summary> Tools</summary>

<details>
<summary> Ruff (0.15.1)</summary>

[error] 405-405: Local variable `sig` is assigned to but never used

Remove assignment to unused variable `sig`

(F841)

</details>

</details>

<details>
<summary> Prompt for AI Agents</summary>

```
Verify each finding against the current code and only fix it if needed.

In `@tests/test_training_validation.py` around lines 401 - 419, Remove the unused
local variable assignment to sig in
test_create_training_args_masking_matches_training_args_default (the line
calling inspect.signature(_create_training_args_via_classmethod)); it is dead
code and not used laterkeep the rest of the test that inspects
BaseGLiNER.create_training_args and compares masking defaults against
TrainingArguments.
```

</details>

<!-- fingerprinting:phantom:medusa:ocelot -->

<!-- This is an auto-generated comment by CodeRabbit -->

---

### coderabbitai[bot] &mdash; 2/17/2026, 7:05:30 PM

<!-- This is an auto-generated comment: summarize by coderabbit.ai -->
<!-- walkthrough_start -->

<details>
<summary> Walkthrough</summary>

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

* **Tests**
  * Enhanced test coverage for training parameter validation and configuration handling.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->
## Walkthrough

Introduces a comprehensive test module validating GLiNER training parameter wiring, defaults, and configuration forwarding. Covers smoke tests, TrainingArguments defaults, custom fields completeness, loss and dataloader wiring, kwargs propagation, and documents known configuration gaps using xfail markers.

## Changes

|Cohort / File(s)|Summary|
|---|---|
|**Training Validation Test Suite** <br> `tests/test_training_validation.py`|New comprehensive test module (684 lines) validating training parameter wiring, TrainingArguments defaults, GLiNER-specific field completeness, loss/dataloader forwarding, kwargs pass-through, and documenting known configuration gaps with xfail markers.|

## Estimated code review effort

 3 (Moderate) |  ~20 minutes

## Poem

>  Hop, skip, and test we go,  
> Parameters wired, high and low,  
> Training defaults checked with care,  
> Config gaps exposed with flair,  
> GLiNER's pipeline shines so bright! 

</details>

<!-- walkthrough_end -->


<!-- pre_merge_checks_walkthrough_start -->

<details>
<summary> Pre-merge checks |  2 |  1</summary>

###  Failed checks (1 warning)

|     Check name     | Status     | Explanation                                                                           | Resolution                                                                         |
| :----------------: | :--------- | :------------------------------------------------------------------------------------ | :--------------------------------------------------------------------------------- |
| Docstring Coverage |  Warning | Docstring coverage is 22.64% which is insufficient. The required threshold is 80.00%. | Write docstrings for the functions missing them to satisfy the coverage threshold. |

<details>
<summary> Passed checks (2 passed)</summary>

|     Check name    | Status   | Explanation                                                                                                                                                          |
| :---------------: | :------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|    Title check    |  Passed | The title directly describes the main change: adding validation tests for training parameter wiring issues, which matches the core changeset.                        |
| Description check |  Passed | The description details specific issues being tested (22 known issues), testing methodology (xfail markers), and provides comprehensive context about the changeset. |

</details>

<sub> Tip: You can configure your own custom pre-merge checks in the settings.</sub>

</details>

<!-- pre_merge_checks_walkthrough_end -->

<!-- finishing_touch_checkbox_start -->

<details>
<summary> Finishing touches</summary>

- [ ] <!-- {"checkboxId": "7962f53c-55bc-4827-bfbf-6a18da830691"} -->  Generate docstrings
<details>
<summary> Generate unit tests (beta)</summary>

- [ ] <!-- {"checkboxId": "f47ac10b-58cc-4372-a567-0e02b2c3d479", "radioGroupId": "utg-output-choice-group-unknown_comment_id"} -->   Create PR with unit tests
- [ ] <!-- {"checkboxId": "07f1e7d6-8a8e-4e23-9900-8731c2c87f58", "radioGroupId": "utg-output-choice-group-unknown_comment_id"} -->   Post copyable unit tests in a comment
- [ ] <!-- {"checkboxId": "6ba7b810-9dad-11d1-80b4-00c04fd430c8", "radioGroupId": "utg-output-choice-group-unknown_comment_id"} -->   Commit unit tests in branch `claude/add-validation-tests-eIXm5`

</details>

</details>

<!-- finishing_touch_checkbox_end -->

<!-- tips_start -->

---



<sub>Comment `@coderabbitai help` to get the list of available commands and usage tips.</sub>

<!-- tips_end -->

<!-- internal state start -->


<!-- DwQgtGAEAqAWCWBnSTIEMB26CuAXA9mAOYCmGJATmriQCaQDG+Ats2bgFyQAOFk+AIwBWJBrngA3EsgEBPRvlqU0AgfFwA6NPEgQAfACgjoCEYDEZyAAUASpETZWaCrKPR1AGxJcAgrXoSaB7wtNTw+Fg0iLjIAGb4fLhU8BgpRDzOaGw0fADu8BRpKIgO0pCQBgByjgKUXACMAKzlBgCqNgAyXLC4uNyIHAD0g0TqsNgCGkzMg864sBSKgwDiHfCVAKI2g9zYHh6DTS2tiHXoFPOL9BUAyvjYFAwkkAJUGAywXAweaNhKs/4wIFgqFxBEwFEYmASABJAAazGagCTCGDOUi4F5vD5cZjaLC3XDUbADfjcMgtADCFBI1Do6E4kAATAAGRkANjArLA9QAHNBmcyOMzGkKAJwALTcCGQtnQ/mQwJCYQikEhyHm1EgShoYiZjMgAGsMPhclgkKU4otmKrYM8kniitxMtlKMhqdwEpoYLbVdIYvYyQx4LF4Awgh55ISKOibc94vsTY7FgIvMwBgYoAxqbSAPr2lJpHNo5DMc1FEgAD24wSDGKdVBdFBJP1qHhziGY+Hw8zSABpILFuPU2f3TnR+0QqLR4Owcx9RAaPSlxBgiP2KNgMDmMFkSP3dohYHn8DnxgJ+2ec52lG2Qv2SMDsMqt9EqDQiLJ78D2zR+hnIAAsmgiAGkUSixL8HgYqWHbUNijDZjQebJKkq5FtGiAALwAOTGuQ2GQBIyDQChaQ+NGjjsFh2FEB4ghBNh/7Up2Ug5puxJ0HO+AeI4GDIMaGKnBiBCQAAYkEpyQAAFKs6xbJA1IAI7YAUZQMMSBDWgIcGwIaJCyIgACU/4bI+z6QOQUh8BufFatgdr4JAMGIOWZlgi+9rvrIgwPkEP4kH+UAACI0rQgzsac9BMBgIbpCGJAeLQJIuQAXiQ7bYNwo7jLEsReHmshkog66YLQLA5rQizcP+LYJe2nbdggq6EcgtVth2XY9mhEFiAkFm7go+xIOEWCFCB/75lu14JU5ZbNdSDhsDmsRWnOtoMIu+DLvYmUehcbh+uqsCavOG0Dn18zSM8AjYEQyDOM8uIUAadIVhB8AeFJr6hrgmEkfZhn2I5l2+tEO3qM8TolGpXiYBGADcA7wBWRSYMUpSQPk+yML8klwlYPg3Dc6AYPQKTTmGNCxgoFALR6pNFG92geE5zgvXwYZYLUCkkCxdAaN6zwOBDKDvDxSjICyGQlEUao2pqiqgs8TC06IGK1MdEjhBQAs+JAwQYAaqrAz61I/DQUWKEL0guSqqC8PgWtKGTkQ+hLWbwNw7kaEY+jGOAUBkPQ+CxDgBDEGQygWworDsFwvD8MIauSGUcgKEoVCqOoWg6H7JhQHAqCoGjvzh6Q5BvnS0xsBgDJULkO1OC4LzyEwGcqGomjaLoYCGP7pgGGqgyQshDpoYrz4aNwsgcAYABEC8GBYkA+DCEcV7S9CLU98gh4wx2rtIRgwrXVzYE890x7wJC2nxKeg9Bih7Hah3D36o8FuPQRKt70/nXwE8VzpFkpsOwk1HTOhIDkTGBRUakwUDFeARAHiT0Fg/ZATArKz3/DcTsL10FcHgMwPa/pSovGAqGUW0RMDiHMnvEiY8iDkWQTXUh8CGEpEoD7AupFVzMMorXZA4FIIxC4FZYM8hhF7AxI+Mo8Q+C4hAn2fW+ASg5mpLQc+7l+zxDDG2OiJQtQkAgtI4qJN6DX1OO8Z4e82r1U6k1OKM5ErcMgBSDSLBIAgPkvFRKGCWDVigZHEoXAyAOGpDzZSql6DeJsGARAgZgyUN8UlSAlYkAYhVBw1CTCKKsPuvAzWzwpFQUQK4joqjkD5EKKuUJfEHjPGyVw6YuwkIGLiAkXIzhUnhi8WsUBKiShgDNpvDIDYglNmNtTTcGcIxFHaSgYhqZ2CT3/EFagQR8BoAzjAmpRAxGUAkTAFCXD0Qfy3KCTZ2zKCQA4sgbgKQry8wSJ+HgroMmzlyAkdmZiyHX1iFAj4y00C9R1v+AA0l0jC0tEAQgWPcIgnw0n1IiZWasoZ1CGihXdaSdiOqNWUYOYco4SB0EBg7J0RBaRTOyWRPJVFCLwDQIwH4JRsiwEUAhGk3t/wAAlRJGJMaUrU+B1KsOfF8NAXsGmQH5ccxh/D8mCpEcgKSzFHbpQipxJgPFmC2X+iQQGZC8UNS6kQYFoLlXSJeDfNAWsEiuLhO9DwYAbpEDAJTD4stDpcAwJRRYxJ0Gs2eq9Z1X0kg/T+huQ1IqxXsCKEaE0WAqX9GkiQDQRAND3irDWTF9Zdw5GQCkLluZwFoWLP2RRoFmolJiOuZ5rEtW0C4rq2yB9aAGzXGktyI17CeRIB+BBsUUHaJ5otdKK1yqnU2ttBw3ASH9iUNsodSCkYJSSv2MhnZqSGVcVSblJBaV8OhS0rwNByAhMIoc2IrdKCEmLR7cQei11+POM8NFuaMQPsQulMt5riyI1xBgeQaK1bPhLGzUNzM7IOQHFQUYwRcC71DgAKhQwabFyB22dtcSFbZgxoqxTACkoxOp3JcDltqNWL7UkPoiLFDQsgsgsw1NBNA8huZLuDhzCI47g4YAjIy5lTMPrBp+eUlQM18VmsGsEW2o0kAGgo36f+PBuzxqCLJ4aKpai4FyKSrAsT9aSfaqaxx5jZUCpNQ4wsPUCB5DGJAETHhXHQBU5g5QpBVMLUojtednps3otrGkqsqiZV73UtETxKTfnwMI0gxjzGjHbOI84+gVGxC9qkmgIgk4B3UvyPMJyCRnjOaMj7AwcAhYqSpg7J2ZRbQeDJJMkSy5FgJOoy5IgO5cANIKVve4jw9wWbuU551YnXRTLKnG2ukAU1HU1LiDjcZkZ0nNlwtBcsdVeEy1IQTZB5FPBC0FvNkDoForC9SfsKtqS6nkVC6cq5N3wIy3WdTtcmUs3i8gt8vaYK4lwPOItrs7S8PSA8skBt01GHMJYHwUEo4jXVCbYpogfi/d4/wUOF2Lh0j6rsFMlD43iCPv+SojkccWx2BMGsyLxBIfsEgnrfXMaUEeooJJdJi09gwQfLzUkedamDKHbZqS5YRARhZRyHxMCkGRzwGnlCfBWBhIz7rRIFr8D4JTukly+0bjEH13dRg1gXv3rLugXAADUbIeQABZBiciMBsaIRDRlt2eNSLWJAG7GPkQyXlSDYDz0XhmXu/dA7wL3qXQg5co5VxYKwrg9dG47xbunZQWcu653D/nGOpZcA5hCIgdRD4Zz6ebdQi4Pc+554AMyMgAOyN+ZLQBgdeeSqFFO35kjfRQN9iLQOvsRGhsnqHbxojJYh27QHX+oI+eT1AYI3mvEf8/qCL0lUv3uK85iDqvvP18nnRnStOkvVeMR+wMAAbwMOUOeSBbAACE6IbToBSRP7ArCqItnPLgEEPBThew79IA55Dx7hEoX9RUDRbA/8BwJI9wQCH9EAAB5KyQofwMgOAgAoApA6cWgGwTcIKUVG4CNVcRACkdaA0OApIeyYA+/fAwgjAdwXALwSghcGg6Neg0Axgog6QD2L2EadgjaTgugpAg2F6WgGEEoeyRAUgigOAhebgueVlXAYQg0GwaQUxOAgAbRAPKFv3KCMNAOnUqF3EUJYK8H3g4O4OMLAMJF60QFEMQOMPvw/UwGfAsJ9HpysOnFu1YMkX4MKFqCOkejxHN0Pi4FFyKEAV7TlnkVVDBzGQLRuWqSKHNFkP7FyAQA+FZkB1tFCJpmVj5yuk0DnlsKMLnmmkUKhRyXKP0MqNu3oyQQaWwIQIqPvwSCQRSCCHULMLYEUJ8JIDngaIAF8KjDDXC55TDzCuA54QpEABD3JrCRCOjQDqFHDnC1i553CesRovDUdFjChBCVRtRmZkAOsgwQwGB0ZZCbVvVog6QpJGR9RE1TRbjpBDJ+xIQih2VFBuJ8BB0pJnMJsmwviLM6sQg1IAlqRb4XIpAEEaAKwMQVB7hhIfQZdD4hINB6jXDQDqi5jai0hcSpimjEEfthj/92iGjOjChRgdwPA+jZieCgjPZ3IRjjDxiGjJi7CZiBi5jiCGBvpmoP8rIcthjtiNjiQ2jACXCpjdjPCBTRVhT0gPN4NnhUAXiNA2Q7cABSTGHI3SIuepXKDFdgAWKrSJFSDRG0BaDlRKYoSAHkZkDQAUXUnE7YhabiPAfYuYgAdUKCpmmxVI6USB9FiE3Ey0x2cllltGtBEkQDCEQBvWpjVPFNtOkHtNoA9JpPxKthqOcDqO2K6PpN6KoP6MpJ4OVLILug5KMNGJAIAF1lDVDbAFiljfTQD6gSA2Rp9YgBA7cZ9R8SAeQeRu87daAeQh82Q0BYgeQ0BZy68GBGQBAeRG9GRly68aQngQVR8687du9RRGRRQSB6hG87cGBx869RRG8SSVDgJcBbBLDKy55mQZzG959G8O9aBGg7cBBhRRBfz6hryRyJ9GhhQ68G87dcpVyeR+zLySABBGQ7dezgLR8LyGApyGAAU68ZyGA7zgyazRTPNYRa5KAGTSDaQ4CeTQCh4R4/0cxYiIgp5ZBqLcy54CBCQPBRJIz3InCuBGg69tiIz3g+K/SxhBSQy4D6hGQxiQCGyGyDBD9qRj9SA1oFwS99884A4w4TwnQOIfxcwL9V8b97zogrBcY6AfBcBNCd939E91AP9NxcA/9mRFK88Y8cx9LThDKkItLe4gA= -->

<!-- internal state end -->

---

### gemini-code-assist[bot] &mdash; 2/17/2026, 7:05:27 PM

## Summary of Changes

Hello @arthrod, I'm Gemini Code Assist[^1]! I'm currently reviewing this pull request and will post my feedback shortly. In the meantime, here's a summary to help you and other reviewers quickly get up to speed!

This pull request significantly enhances the robustness of the GLiNER training system by introducing a dedicated suite of validation tests. The primary goal is to proactively identify and document existing or potential misconfigurations and wiring issues related to training parameters, ensuring greater stability and predictability in model training. By using `xfail` markers, the PR clearly outlines areas needing attention without blocking the test suite, providing a clear roadmap for future improvements.

### Highlights

* **New Validation Test Suite**: Introduced a comprehensive test suite, `test_training_validation.py`, to validate the correct wiring and configuration of training parameters within the GLiNER framework.
* **Documentation of Known Issues**: Many tests are marked with `pytest.mark.xfail(strict=True)` to explicitly document and track known issues, such as missing explicit parameters in `create_training_args`, masking default mismatches, and `remove_unused_columns` not being set to `False`.
* **Parameter Wiring Verification**: Added tests to ensure critical training parameters, including those for focal loss, label smoothing, and dataloader configurations, are correctly forwarded and consumed throughout the training pipeline.
* **Detection of Dead Config Fields**: Included tests to identify 'dead' configuration fields (e.g., `size_sup`, `shuffle_types`, `random_drop`) that exist in `config.yaml` but are not actively used in the codebase.
* **Evaluation and Checkpoint Support**: Added `xfail` tests highlighting that evaluation might not run due to misconfigured `evaluation_strategy`/`eval_steps` and that `train_model` currently lacks explicit support for `resume_from_checkpoint`.

<details>
<summary><b>Changelog</b></summary>

* **tests/test_training_validation.py**
    * Added a new test file to validate training parameter wiring and configuration.
    * Implemented smoke tests for basic imports and instantiation of `TrainingArguments` and `Trainer`.
    * Verified default values for GLiNER-specific `TrainingArguments` fields like `masking`, `label_smoothing`, and focal loss parameters.
    * Ensured all required GLiNER-specific fields are declared in `TrainingArguments` and have defaults.
    * Confirmed that `Trainer.compute_loss` correctly forwards all loss-related parameters.
    * Validated that `Trainer` dataloader methods utilize `pin_memory`, `persistent_workers`, and `prefetch_factor`.
    * Tested `kwargs` pass-through integrity for `create_training_args` to ensure parameters like `label_smoothing`, `fp16`, and `seed` are correctly applied.
    * Documented HF default behaviors for `remove_unused_columns` and `label_smoothing_factor` that pose risks for GLiNER.
    * Added `xfail` tests for `create_training_args` to highlight missing explicit parameters (e.g., `label_smoothing`, `fp16`, `seed`, `evaluation_strategy`).
    * Included an `xfail` test for a masking default mismatch between `create_training_args` and `TrainingArguments`.
    * Created `xfail` tests to address `remove_unused_columns` not being set to `False` and its absence in `train_model` source.
    * Added `xfail` tests to verify proper evaluation configuration, including `evaluation_strategy` and `eval_steps`.
    * Introduced `xfail` tests to detect 'dead' config fields (`size_sup`, `shuffle_types`, `random_drop`) that are not consumed by the codebase.
    * Included an `xfail` test to identify potential name collision between GLiNER's `label_smoothing` and HF's `label_smoothing_factor`.
    * Added `xfail` tests to confirm `train_model` accepts and forwards `resume_from_checkpoint`.
</details>

<details>
<summary><b>Using Gemini Code Assist</b></summary>
<br>

The full guide for Gemini Code Assist can be found on our [documentation page](https://developers.google.com/gemini-code-assist/docs/review-github-code), here are some quick tips.

<b>Invoking Gemini</b>

You can request assistance from Gemini at any point by creating a comment using either `/gemini <command>` or `@gemini-code-assist <command>`. Below is a summary of the supported commands on the current page.

Feature | Command | Description
--- | --- | ---
Code Review | `/gemini review` | Performs a code review for the current pull request in its current state.
Pull Request Summary | `/gemini summary` | Provides a summary of the current pull request in its current state.
Comment | @gemini-code-assist | Responds in comments when explicitly tagged, both in pull request comments and review comments.
Help | `/gemini help` | Displays a list of available commands.

<b>Customization</b>

To customize Gemini Code Assist for GitHub experience, repository maintainers can create a configuration file and/or provide a custom code review style guide (such as PEP-8 for Python) by creating and adding files to a `.gemini/` folder in the base of the repository. Detailed instructions can be found [here](https://developers.google.com/gemini-code-assist/docs/customize-gemini-behavior-github).

<b>Limitations & Feedback</b>

Gemini Code Assist may make mistakes. Please leave feedback on any instances where its feedback is incorrect or counter productive. You can react with :thumbsup: and :thumbsdown: on @gemini-code-assist comments. If you're interested in giving your feedback about your experience with Gemini Code Assist for Github and other Google products, sign up [here](https://google.qualtrics.com/jfe/form/SV_2cyuGuTWsEw84yG).

<b>You can also get AI-powered code generation, chat, as well as code reviews directly in the IDE at no cost with the [Gemini Code Assist IDE Extension](https://cloud.google.com/products/gemini/code-assist).</b>
</details>


[^1]: Review the [Privacy Notices](https://policies.google.com/privacy), [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy), [Terms of Service](https://policies.google.com/terms), and learn how to configure Gemini Code Assist in GitHub [here](https://developers.google.com/gemini-code-assist/docs/customize-gemini-behavior-github). Gemini can make mistakes, so double check it and [use code with caution](https://support.google.com/legal/answer/13505487).


---

